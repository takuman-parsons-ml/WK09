{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "# Week 09\n",
        "\n",
        "Unsupervised Learning: PCA and Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this week's exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py\n",
        "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/image_utils.py\n",
        "\n",
        "!wget -qO- https://github.com/PSAM-5020-2025S-A/5020-utils/releases/latest/download/att-faces.tar.gz | tar xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import PIL.Image as PImage\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from data_utils import PCA, RandomForestClassifier, StandardScaler\n",
        "from data_utils import KMeansClustering\n",
        "from data_utils import object_from_json_url, classification_error, display_confusion_matrix\n",
        "\n",
        "from image_utils import get_pixels, make_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Unsupervised Learning\n",
        "\n",
        "### Dimensionality Reduction\n",
        "\n",
        "A few homeworks ago we looked at an image classification exercise and we saw that creating image filters by hand and selecting which features to look at can be a very laborious and complex process. Let's now combine what we learned about data normalization, distances, and learning algorithms to try to create a better image classification system.\n",
        "\n",
        "Consider the following image that is $4$ pixels wide by $4$ pixels high, it has a total of $16$ pixels:\n",
        "\n",
        "<img src=\"./imgs/pixdim-00.jpg\" height=\"200px\" />\n",
        "\n",
        "We can think of these $16$ pixels as the features of the image, and if we had a dataset of images of this size, we can think of the rows in this dataset as our different images and then the columns being the $16$ values for each of the pixels of those images.\n",
        "\n",
        "And the same is true for a dataset that has images that are $256$ by $256$ pixels:\n",
        "\n",
        "<img src=\"./imgs/pixdim-01.jpg\" height=\"200px\" />\n",
        "\n",
        "We would just have as many rows as we have images, and each of those rows would have $65,536$ columns/features, one for each of its pixels.\n",
        "\n",
        "We might be tempted now to develop some kind of method that uses $L2$ distances or cosine similarities to classify images by finding similarities between their pixel content. And that might work sometimes, but consider that computing the $L2$ distance between $2$ of our $256$ x $256$ pixel images would require squaring over $65000$ terms... and if we look at the black and white $4$ x $4$ images below, it's not so clear that subtracting raw pixel values would give us any indication of similarity:\n",
        "\n",
        "<img src=\"./imgs/pixdiff.jpg\" height=\"200px\" />\n",
        "\n",
        "The difference between the first image and the second image is of $8$ pixels. The difference between the first and third image is also of $8$ pixels, while the difference between the second and third image is of $16$ pixels, even though they're more similar. The same could be calculated for the last two pictures and the first.\n",
        "\n",
        "What we want to do instead is something called **_dimensionality reduction_**, where we turn those $65,000$ columns of pixel values into a smaller number of columns that represent more meaningful and complex information about the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Projection\n",
        "\n",
        "One way of reducing the dimensions of our dataset is by doing something called **_feature projection_**, which is very simple to perform and understand. We did this when we dropped features from the wine dataset last week.\n",
        "\n",
        "**_Feature Projection_** is when we just drop the columns in our dataset that we think are less important. The results of doing this can be seen in the following two images:\n",
        "\n",
        "<img src=\"./imgs/dimredproj-00.jpg\" height=\"350px\" />\n",
        "\n",
        "On the left side we have a graph of our data set in two dimensions. On the right, at the very bottom, we've projected all of those points straight down onto the x-axis by just ignoring their $y$ components.\n",
        "\n",
        "On next image we can see what the problem with doing that might be. Points that are actually further apart in our full dataset will seem closer once projected to a lower dimension like this, and points that are close together in $2D$ space can seem further apart in $1D$ space.\n",
        "\n",
        "<img src=\"./imgs/dimredproj-03.jpg\" height=\"350px\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Space-Filling Curves\n",
        "\n",
        "Another way of reducing dimensions is by using **_space-filling curves_**, like the [Hilbert curve](https://en.wikipedia.org/wiki/Hilbert_curve) or [Z-order curves](https://en.wikipedia.org/wiki/Z-order_curve).\n",
        "\n",
        "These functions reorder our data by tracing a curvy path between all the points in our dataset. If we look at the following images:\n",
        "\n",
        "<img src=\"./imgs/dimredcurve-00.jpg\" height=\"350px\" />\n",
        "\n",
        "The image on the left again shows our $2D$ data points graphed on a plane. On the right, we've drawn a line that connects all of our points by starting with a point on the lower-left side of the graph and then connecting to the nearest point, and then the next nearest point and so on and so on. In the end we have the light-blue line that connects and re-orders all of our $2D$ points into a $1D$ line.\n",
        "\n",
        "The problem with this method is that **_space-filling curves_** still have some discontinuities, or jumps, which might result in the same problem as **_projection_**, where closer points become further apart after transformation and vice-versa.\n",
        "\n",
        "### Principal Component Analysis\n",
        "\n",
        "One final way of reducing the dimensions in our dataset is by doing something called **_principal component analysis_** or **_PCA_**. This is a kind of unsupervised learning algorithm that looks at all of our original data and tries to figure out the directions in which it spreads out the most.\n",
        "\n",
        "<img src=\"./imgs/dimredpca-00.jpg\" height=\"350px\" />\n",
        "\n",
        "Again, on the left we have the original $2D$ data on a plane. On the right we see the original data with a line that represents the direction in which the data seems to spread.\n",
        "\n",
        "**_PCA_** is kind of, sort of, like doing **_Linear Regression_** in a special manner, and finding many lines-of-best-fit, but with special properties. We can think of these new lines as new axes that we re-orient our data around, and instead of having the $x$ axis and the $y$ axis we'll have $2$ new axes, one in the direction that captures the most variation in our data, and another in a perpendicular direction, that captures less of the variation in our data, like below:\n",
        "\n",
        "<img src=\"./imgs/dimredpca-03.jpg\" height=\"350px\" />\n",
        "\n",
        "But, wait ! If we have $N$ dimensions to begin with and we end up with these $N$ lines/axis/features, isn't it the same as before ? We still have $N$ dimensions to deal with!\n",
        "\n",
        "Yes, but the thing about the **_PCA_** axes is that these lines, or, principal components, are actually ordered by importance, so the very first principal component is the one that captures most of the variation in our data, the second one captures the second most amount of variation in our data, the third one the third most, and so on and so on...\n",
        "\n",
        "What we want to do after we compute the principal components of our dataset is perform a **_projection_** and chop off most of the new dimensions. Most of the information about our data will be captured by the first couple of principal components, so we can now project our data onto a few of these new axes and they'll retain most of the information about our data.\n",
        "\n",
        "<img src=\"./imgs/dimredpca-02.jpg\" height=\"350px\" />\n",
        "\n",
        "In the $2D$ case above if we only consider the first principal component and project our data onto that blue line we retain most of the information about the relative distances between the points in our dataset, but since they're all projected onto a line, they can be represented by just $1$ feature/value. In the end we were able to reduce our data from having $2$ variables in the $XY$ plane to just having one variable in the direction of the first principal component.\n",
        "\n",
        "Let's look at some examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCA in 3D\n",
        "\n",
        "Let's reload the `ANSUR` data and look only at the features about hands. We'll use PCA to reduce the dimensions of this subset of our data from $3$ to $1$ dimension.\n",
        "\n",
        "We'll start by looking at the hand data using $3$ features because $3$ is something we can plot using $3D$ graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ANSUR_FILE = \"https://raw.githubusercontent.com/PSAM-5020-2025S-A/5020-utils/main/datasets/json/ansur.json\"\n",
        "\n",
        "ansur_data = object_from_json_url(ANSUR_FILE)\n",
        "\n",
        "ansur_df = pd.json_normalize(ansur_data)\n",
        "display(ansur_df.head())\n",
        "\n",
        "hand_df = ansur_df[[\"hand.breadth\", \"hand.length\", \"hand.palm\"]]\n",
        "\n",
        "# scale and pca objects\n",
        "hand_mss = StandardScaler()\n",
        "hand_pca = PCA(n_components=1)\n",
        "\n",
        "# perform scaling and pca\n",
        "hand_df = hand_mss.fit_transform(hand_df)\n",
        "hand_pca_df = hand_pca.fit_transform(hand_df)\n",
        "\n",
        "# project from 1 PCA back to original 3 dimensions\n",
        "hand_pca_i_df = hand_pca.inverse_transform(hand_pca_df)\n",
        "\n",
        "# plot\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "# Original 3D data in x,y,z space\n",
        "ax.scatter(hand_df[\"hand.breadth\"], hand_df[\"hand.length\"], hand_df[\"hand.palm\"], s=3, alpha=0.15)\n",
        "\n",
        "# Original data projected onto each of the 3 axis\n",
        "ax.scatter(hand_df[\"hand.breadth\"], [-3.5]*len(hand_df[\"hand.length\"]), [-3.5]*len(hand_df[\"hand.palm\"]), s=3, c='#ff8000', alpha=0.333)\n",
        "ax.scatter([-3.5]*len(hand_df[\"hand.breadth\"]), hand_df[\"hand.length\"], [-3.5]*len(hand_df[\"hand.palm\"]), s=3, c='#ff8000', alpha=0.333)\n",
        "ax.scatter([-3.5]*len(hand_df[\"hand.breadth\"]), [-3.5]*len(ansur_df[\"hand.length\"]), hand_df[\"hand.palm\"], s=3, c='#ff8000', alpha=0.333)\n",
        "\n",
        "# Data projected onto the first principal component\n",
        "ax.scatter(hand_pca_i_df[\"hand.breadth\"], hand_pca_i_df[\"hand.length\"], hand_pca_i_df[\"hand.palm\"], s=3, c='r', alpha=0.333)\n",
        "\n",
        "# Axis limits\n",
        "ax.set_xlim((-4, 4))\n",
        "ax.set_ylim((-4, 4))\n",
        "ax.set_zlim((-4, 4))\n",
        "\n",
        "# Axis Labels\n",
        "ax.set_xlabel(\"Hand Breadth\")\n",
        "ax.set_ylabel(\"Hand Length\")\n",
        "ax.set_zlabel(\"Palm Length\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graph above shows the original data for the `hand.breadth`, `hand.length` and `hand.palm` features in blue.\n",
        "\n",
        "The orange points show the data projected onto the different axis. It is $1$ dimensional, meaning that in each of those cases we've completely ignored two features.\n",
        "\n",
        "The red points show our data projected onto the first principal component.\n",
        "\n",
        "If we call the `pca.explained_variance()` function we can see how much of our data's information has been retained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hand_pca.explained_variance()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCA in all dimensions\n",
        "\n",
        "What if we reduce all of our features to just $1$? How much of the data's information is kept?\n",
        "\n",
        "If we want to keep $80\\%$ of our data's variance, how many components should our PCA use?\n",
        "\n",
        "No need to plot anything, just run pca a few times and look at its `explained_variance`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Repeat above process, using all of the original features, except gender\n",
        "mss = StandardScaler()\n",
        "ansur_df = pd.json_normalize(ansur_data).drop(columns=[\"gender\"])\n",
        "ansur_df = mss.fit_transform(ansur_df)\n",
        "\n",
        "## TODO: get explained_variance() for n_components=1\n",
        "\n",
        "## TODO: find n_components for explained_variance >= 0.80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "It takes more than $3$ components to keep $80\\%$ of the information from our data. This isn't something we can easily plot to compare to the distribution of the original $14$ features.\n",
        "\n",
        "We can plot the first $2$ and first $3$ principle components, but after that we just have to trust the `explained_variance` number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### More PCA in Multiple Dimensions\n",
        "\n",
        "The above exercises are good for us to start building intuition about PCA, but even though we can perform PCA and reduce our features from $15$ to $5$ or $6$ principal components while retaining most of the variation in our data, it gets hard to visualize the effects of these transformations.\n",
        "\n",
        "Let's use images. We'll load images with about $10\\text{,}000$ pixels/features, and perform PCA to represent them using only $10$ components. Then we'll remap our $10$ components for each image back to $10\\text{,}000$ pixels to look at the effects of **_compressing_** our data like this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCA on Images\n",
        "\n",
        "Let's load all of the images inside the `data/image/att-faces/` directory. These images are in sub-directories that specify the `id` of the person in the image: `s0` is `subject 0`, `s1` is `subject 1`, and so on.\n",
        "\n",
        "We'll keep track of these ids and create numeric labels for them as we read the data from the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lists for keeping track of image pixel lists, subject numeric id and subject label\n",
        "face_pixels = []\n",
        "face_ids = []\n",
        "id2label = []\n",
        "\n",
        "# 40 directories\n",
        "for l in range(1, 41):\n",
        "  id2label.append(f\"s{l}\")\n",
        "  # 10 images per directory\n",
        "  for i in range(1, 11):\n",
        "    mimg = PImage.open(f\"./data/image/att-faces/s{l}/{i}.pgm\")\n",
        "    face_pixels.append(get_pixels(mimg))\n",
        "    face_ids.append(l)\n",
        "\n",
        "# display first image\n",
        "display(make_image(face_pixels[0], width=92))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print len of lists to make sure sizes match\n",
        "print(len(face_pixels), len(face_ids))\n",
        "\n",
        "# check number of labels matches number of directories\n",
        "print(len(id2label))\n",
        "\n",
        "# check how many pixels per image and look at first 10 pixel values\n",
        "print(len(face_pixels[0]), face_pixels[0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These look ok. There's only one color value per pixel since these are grayscale images.\n",
        "\n",
        "#### Run PCA\n",
        "\n",
        "We can run PCA directly on these lists of pixels. We don't even have to scale the data because we know the pixel values are all between $0$ and $255$ for greyscale images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run pca and get first 10 PCs\n",
        "pca = PCA(n_components=10)\n",
        "faces_df = pca.fit_transform(face_pixels)\n",
        "\n",
        "# put id values in DataFrame for classification later\n",
        "faces_df[\"id\"] = face_ids\n",
        "\n",
        "display(faces_df.head())\n",
        "\n",
        "print(pca.explained_variance())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation\n",
        "\n",
        "Our PCA-transformed data retained about $60\\%$ of the information from the original data.\n",
        "\n",
        "Our `DataFrame` now is expressed in terms of the first $10$ principal components of our data. The values for these new features are all over the place, and don't represent pixel colors anymore. They represent our data in a more _abstract_, _latent_, space. It's latent because the information is there, it exists, but it's sort of waiting to be transformed back into physical values and, in this case, visualized as pixels.\n",
        "\n",
        "This `PCA` transformation was able to compress our data and reduce the number of dimensions for each of our images from $10\\text{,}304$ to $10$ ! That's a reduction factor of $1000$ ! We could start thinking about doing clustering or classification by measuring distances between these images because now they only have $10$ dimensions, and each of the dimensions represents a combination of color information, not just one pixel.\n",
        "\n",
        "But, let's visualize our new compressed images first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reconstruction: un-projects the data from PCA space into pixel space\n",
        "pca_pixels = pca.inverse_transform(faces_df)\n",
        "\n",
        "display(make_image(face_pixels[0], width=92))\n",
        "display(make_image(pca_pixels.loc[0], width=92))\n",
        "\n",
        "display(make_image(face_pixels[110], width=92))\n",
        "display(make_image(pca_pixels.loc[110], width=92))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ¤”\n",
        "\n",
        "We did lose $40\\%$ of the detail of our data when we transformed it into `PCA` space.\n",
        "\n",
        "These don't look too good at first, but they also don't look that bad. Given how regular all of the images are, maybe this level of detail is all that is needed to do any kind of classification or clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### More components\n",
        "\n",
        "Re-run PCA with $20$ and $120$ components. Don't worry about printing the `DataFrame`, but reconstruct some of the images like we did in the cell above.\n",
        "\n",
        "What is the effect of keeping more components in our PCA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: reconstruct images using 20 component PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: reconstruct images using 120 component PCAs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Common features\n",
        "\n",
        "One way to think about how `PCA` compresses our features, is to imagine that it can factor out common elements between features that are strongly correlated. For numerical data, those common factors are just parameters to a series of multiplications. Boring.\n",
        "\n",
        "With images, the common factors between our features can actually be seen because our features represent pixels.\n",
        "\n",
        "We can iterate over the `PCA` factors (or, `components`) and display them as images. We just have to scale them to pixel range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for comp in pca.components[:5]:\n",
        "  minVal = comp.min()\n",
        "  maxVal = comp.max()\n",
        "  # manually mapping to [0, 1]\n",
        "  pxs01 = (comp - minVal) / (maxVal - minVal)\n",
        "  # mapping to [0, 255]\n",
        "  pxs = [255 * p for p in pxs01]\n",
        "  display(make_image(pxs, width=92))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "Having $10$ components in our transformation means having $10$ of these \"common\" faces and $10$ features in our transformed dataset. The original faces are reconstructed (with error) by multiplying the $10$ features in the transformed dataset by their corresponding face, so `PC0` with the first face, `PC1` with the second face, and so on, and then adding those results.\n",
        "\n",
        "These top-5 \"common\" faces will always be the same for this dataset, independent of the number of components we keep in our `PCA` transformation. \n",
        "\n",
        "Adding components to our transformation just increases the number of these \"common\" faces that get extracted, but their order doesn't change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Back to $10$ components\n",
        "\n",
        "Let's go back to just using $10$ PCs and see how the first $100$ images are distributed in the first $2$ dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run pca and get first 10 PCs\n",
        "pca = PCA(n_components=10)\n",
        "faces_df = pca.fit_transform(face_pixels)\n",
        "\n",
        "# put id values in DataFrame\n",
        "faces_df[\"id\"] = face_ids\n",
        "\n",
        "print(pca.explained_variance())\n",
        "\n",
        "# first 10 ids\n",
        "x = faces_df[\"PC0\"][:100]\n",
        "y = faces_df[\"PC1\"][:100]\n",
        "z = faces_df[\"PC2\"][:100]\n",
        "c = faces_df[\"id\"][:100]\n",
        "\n",
        "# 2D\n",
        "plt.scatter(x, y, c=c, marker='o', linestyle='', alpha=1, cmap=\"tab10\")\n",
        "plt.title(\"Principal Components\")\n",
        "plt.xlabel(\"PC 0\")\n",
        "plt.ylabel(\"PC 1\")\n",
        "plt.show()\n",
        "\n",
        "# 3D\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c=c, marker='o', linestyle='', alpha=1, cmap=\"tab10\")\n",
        "plt.title(\"Principal Components\")\n",
        "ax.set_xlabel(\"PC 0\")\n",
        "ax.set_ylabel(\"PC 1\")\n",
        "ax.set_ylabel(\"PC 1\")\n",
        "\n",
        "ax.set_xlim((-3000, 3000))\n",
        "ax.set_ylim((-500, 3000))\n",
        "# ax.set_zlim((-4, 4))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation\n",
        "\n",
        "Here we are only looking at the top $2$ and $3$ dimensions of our data and we can already see some patterns/clusters.\n",
        "\n",
        "The colors were added manually using the correct `id` for each image, so that helps to see patterns...\n",
        "\n",
        "Now, let's run the `RandomForestClassifier` algorithm on this dataset to see if PCA dimensionality reduction can actually help prepare image datasets for classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA + Classification\n",
        "\n",
        "We have already prepared our `DataFrame` and added a column with the numerical `id` of each image.\n",
        "\n",
        "We can check by running `faces_df.head()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "faces_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Separate Train and Test Data\n",
        "\n",
        "In order to measure how well our classifier works in general, we should separate our data into $2$ subsets, one which will get used to train our model, and another that we can use to measure how well our model performs on data it hasn't seen before.\n",
        "\n",
        "This is to avoid having our model memorize the training data, and then performing horrible on new data in the future.\n",
        "\n",
        "We'll use the `Scikit-Learn` function `train_test_split` to split our `DataFrame` into $2$ equal datasets.\n",
        "\n",
        "Since the split is random, the `while` loop below is just to guarantee that our test dataset has images from all subjects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_ids = []\n",
        "\n",
        "while len(set(test_ids)) != len(id2label):\n",
        "  faces_train_df, faces_test_df = train_test_split(faces_df, test_size=0.5)\n",
        "  test_ids = faces_test_df[\"id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(faces_train_df.head())\n",
        "display(faces_test_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Classifier\n",
        "\n",
        "The below steps should look familiar.\n",
        "\n",
        "We split the output feature from he rest of the features, and pass those to the `fit()` function of an instance of the `RandomForestClassifier` class.\n",
        "\n",
        "We're not re-scaling our components/features from `PCA`, because we're assuming that any difference in the magnitude of those features is actually important information that the algorithm can use to create a model. And, `RandomForest` and `DecisionTree` classifiers tend to be a little less sensitive to scaling because of their `if`/`else` nature.\n",
        "\n",
        "That's not always the case. Other modeling methods/algorithms might benefit from having standardized data to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "face_model = RandomForestClassifier()\n",
        "\n",
        "train_features = faces_train_df.drop(columns=[\"id\"])\n",
        "train_ids = faces_train_df[\"id\"]\n",
        "\n",
        "# Create a model that classifies faces based on principal components\n",
        "face_model.fit(train_features, train_ids)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "train_predicted = face_model.predict(train_features)\n",
        "\n",
        "## 7. Measure error\n",
        "classification_error(train_ids, train_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification Error should be close to $0$.\n",
        "\n",
        "#### Test Data\n",
        "\n",
        "Now we'll run the `predict()` function of our classifier on our test dataset and see how it performs.\n",
        "\n",
        "Again, this should look familiar: we just have to remember to separate the output feature (`id`) from the other features, and then pass the list of independent features to the `predict()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_features = faces_test_df.drop(columns=[\"id\"])\n",
        "test_ids = faces_test_df[\"id\"]\n",
        "\n",
        "## 6. Run the model on the test data\n",
        "test_predicted = face_model.predict(test_features)\n",
        "\n",
        "## 7. Measure error\n",
        "classification_error(test_ids, test_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation\n",
        "\n",
        "The Classification Error should be close to $20\\%$.\n",
        "\n",
        "This isn't bad !\n",
        "\n",
        "Remember, we just loaded up some unknown dataset of images, automatically reduced the number of dimensions in each image from $10\\text{,}304$ to $10$ and used a basic `RandomForestClassifier()` to learn patterns on the compressed data.\n",
        "\n",
        "We didn't have to look at pixels, we didn't have to manually filter images, we didn't even have to guess which features to keep. PCA helped with all of this.\n",
        "\n",
        "#### Confusion Matrix\n",
        "\n",
        "We can run the following cell to print a **_Confusion Matrix_** for our model. This is a graph that shows how well our model performed on each class of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_confusion_matrix(test_ids, test_predicted, display_labels=id2label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There's probably a class or two that our model didn't do too well with.\n",
        "\n",
        "#### Improving Our Model\n",
        "\n",
        "How can we improve this model ? We picked $10$ components just to see what would happens with a dimensionality reduction factor of $1000$.\n",
        "\n",
        "What happens if we use $20$ components? $20$ is still a lot less than $10\\text{,}304$ and probably still easy for our `RandomForestClassifier` to handle.\n",
        "\n",
        "How much of the data's variance is kept with $20$ components?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TODO: Re-run the classification training, but using 15, 20, 32 PCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_ids = []\n",
        "\n",
        "while len(set(test_ids)) != len(id2label):\n",
        "  faces_train_df, faces_test_df = train_test_split(faces_df, test_size=0.5)\n",
        "  test_ids = faces_test_df[\"id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_features = faces_train_df.drop(columns=[\"id\"])\n",
        "train_ids = faces_train_df[\"id\"]\n",
        "\n",
        "test_features = faces_test_df.drop(columns=[\"id\"])\n",
        "test_ids = faces_test_df[\"id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "face_model = RandomForestClassifier()\n",
        "\n",
        "# Create a model that classifies faces based on principal components\n",
        "face_model.fit(train_features, train_ids)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "train_predicted = face_model.predict(train_features)\n",
        "\n",
        "## 7. Measure error\n",
        "classification_error(train_ids, train_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Run the model on the test data\n",
        "test_predicted = face_model.predict(test_features)\n",
        "\n",
        "## 7. Measure error\n",
        "classification_error(test_ids, test_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation \n",
        "\n",
        "In order to improve the classifier it might not be enough to just increase the number of components. It could be that this is as good as a `RandomForestClassifier` will do when given $10$ to $32$ Principal Component features.\n",
        "\n",
        "Using all possible PC features ($400$), on the other hand, definitely makes our model worse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "Unsupervised learning can be a very powerful and useful tool for performing exploratory data analysis and data pre-processing.\n",
        "\n",
        "Because there's no labeled/correct answer in unsupervised learning, we can be a bit more subjective in how we pick our metrics for success."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.17 ('hf-model')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
